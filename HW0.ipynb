{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nishant Raj - CS685_HW0_F21.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgNZTjrhcHa0"
      },
      "source": [
        "## Homework 0, CS685 Fall 2021\n",
        "\n",
        "### This is due on September 13th, 2021, submitted via Gradescope as a PDF (File>Print>Save as PDF). 100 points total.\n",
        "\n",
        "#### IMPORTANT: After copying this notebook to your Google Drive, please paste a link to it below. To get a publicly-accessible link, hit the *Share* button at the top right, then click \"Get shareable link\" and copy over the result. If you fail to do this, you will receive no credit for this homework!\n",
        "***LINK: https://colab.research.google.com/drive/1f5wIrX7QInoR5hHpgUVcm-KG-OF4Bacj?usp=sharing***\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "##### *How to do this problem set:*\n",
        "\n",
        "- Some questions require writing Python code and computing results, and the rest of them have written answers. For coding problems, you will have to fill out all code blocks that say `YOUR CODE HERE`.\n",
        "\n",
        "- For text-based answers, you should replace the text that says \"Write your answer here...\" with your actual answer.\n",
        " \n",
        "- This assignment is designed so that you can run all cells almost instantly. If it is taking longer than that, you have made a mistake in your code.\n",
        "\n",
        "---\n",
        "\n",
        "##### *How to submit this problem set:*\n",
        "- Write all the answers in this Colab notebook. Once you are finished, generate a PDF via (File -> Print -> Save as PDF) and upload it to Gradescope.\n",
        "  \n",
        "- **Important:** check your PDF before you submit to Gradescope to make sure it exported correctly. If Colab gets confused about your syntax, it will sometimes terminate the PDF creation routine early.\n",
        "\n",
        "- **Important:** on Gradescope, please make sure that you tag each page with the corresponding question(s). This makes it significantly easier for our graders to grade submissions. We may take off points for submissions that are not tagged.\n",
        "\n",
        "- When creating your final version of the PDF to hand in, please do a fresh restart and execute every cell in order. Then you'll be sure it's actually right. One handy way to do this is by clicking `Runtime -> Run All` in the notebook menu.\n",
        "\n",
        "---\n",
        "\n",
        "##### *Academic honesty*\n",
        "\n",
        "- We will audit the Colab notebooks from a set number of students, chosen at random. The audits will check that the code you wrote actually generates the answers in your PDF. If you turn in correct answers on your PDF without code that actually generates those answers, we will consider this a serious case of cheating. See the course page for honesty policies.\n",
        "\n",
        "- We will also run automatic checks of Colab notebooks for plagiarism. Copying code from others is also considered a serious case of cheating.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jxa48WP8dXh9"
      },
      "source": [
        "##Question 1.1 (10 points)\n",
        "Let's begin with a quick probability review. In the task of language modeling, we're interested in computing the **joint** probability of some text. Say we have a sentence $s$ with $n$ words ($w_1, w_2, w_3, \\dots, w_n$) and we want to compute the joint probability $P(w_1, w_2, w_3, \\dots, w_n$). Assume we are given a model that produces the conditional probability of the next word in a sentence given all preceding words: $P(w_i|w_1,w_2,\\dots,w_{i-1})$. How can we use this model to compute the joint probability of sentence $s$?\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyrvDudjbPUO"
      },
      "source": [
        "Bayes Rule of probability states that $P(A,B)$ = $P(A|B)*P(B)$. Similarly, $P(A,B,C)$ = $P(C|B,A)*P(B|A)*P(A)$ and so on. Thus, we can leverage chain rule for a sequence of words to estimate the joint probability of a given sequence by multiplying together their conditional probabilities:\n",
        "\n",
        "$P(w_1, w_2, w_3, \\dots, w_n)$ = $P(w_1)*P(w_2|w_1)*P(w_3|w_1,w_2)*\\dots*P(w_n|w_1,w_2,\\dots,w_{n-1})$\n",
        "\n",
        "> So basically, we start with base case i.e. $P(w1)$ from the model, and use that given model to keep getting the probability of next word given we have previous words and in the end multiply all these probabilites from the model to compute the final joint probability of the given sentence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsq0NIDzcfeC"
      },
      "source": [
        "##Question 1.2 (10 points)\n",
        "Okay, but why would we ever want to compute the joint probability of a sentence? Provide **two** different reasons why this probability might be useful to solve an NLP task.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JLR5ML-dQ2t"
      },
      "source": [
        "\n",
        "1. Joint probability helps to decide what sequence of words would make more sense in a language since it uses conditional probability. This is one of the reasons why it is being used in Machine Translation systems during the decoding process. \"I will go to the club\" will have a higher joint probability of occurence than \"I  will fly to club the\".  \n",
        "2.  It might help to identify texts in an extremely noisy environment and helps to identify ones that best fit in. e.g. \"I saw a van\" (example taken from class discussion) would have a better probability than \"eyes awe of an\" in speech recognition tasks where we have to string together phonemes to identify words in order to make a coherent sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM2aVbUcYPXv"
      },
      "source": [
        "##Question 1.3 (5 points)\n",
        "Here is a simple way to build a language model: for any prefix $w_1, w_2, \\dots, w_{i-1}$, retrieve all occurrences of that prefix in some huge text corpus (such as the [Common Crawl](https://https://commoncrawl.org/)) and keep count of the word $w_i$ that follows each occurrence. I can then use this to estimate the conditional probability $P(w_i|w_1, w_2, \\dots, w_{i-1})$ for any prefix. Explain why this method is completely impractical!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crODlcAKfGcu"
      },
      "source": [
        "*   \"Too many possible sentences\": Even though we have a very big corpus, we can always get a word that was not in vocabulary by replacing one of the words in a given sentence resulting in a zero count in training set and thus zero joint probability overall for that sentence. So, there is no way we can create a corpus which has every possible sentence in it.\n",
        "*   The bigger the n-gram, more  is the compute and space required. The issue gets problematic when the sequence of words is large. e.g. \"Alfred is master at playing Chess\". In this case to compute probability of this sentence, we will have to find the sum of counts of all the possible sequences from these 6 words apart from calculating the count of occurence of given sentence in the corpus which results in huge computations and this problem grows exponentially with increasing length of the sentence.\n",
        "\n",
        "Hence, this method is impractical.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1_cCBOwfPxI"
      },
      "source": [
        "##Question 2.1 (5 points)\n",
        "Let's switch over to coding! The below coding cell contains the opening paragraph of Daphne du Maurier's novel *Rebecca*. Write some code in this cell to compute the number of unique word **types** and total word **tokens** in this paragraph (watch the lecture videos if you're confused about what these terms mean!). Use a whitespace tokenizer to separate words (i.e., split the string by white space). Be sure that the cell's output is visible in the PDF file you turn in on Gradescope.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9Fm6AQJQDFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b4d56d9-cd93-4431-98d6-a221b46fcb14"
      },
      "source": [
        "paragraph = '''Last night I dreamed I went to Manderley again. It seemed to me\n",
        "that I was passing through the iron gates that led to the driveway.\n",
        "The drive was just a narrow track now, its stony surface covered\n",
        "with grass and weeds. Sometimes, when I thought I had lost it, it\n",
        "would appear again, beneath a fallen tree or beyond a muddy pool \n",
        "formed by the winter rains. The trees had thrown out new\n",
        "low branches which stretched across my way. I came to the house\n",
        "suddenly, and stood there with my heart beating fast and tears\n",
        "filling my eyes.'''.lower() # lowercase normalization is often useful in NLP\n",
        "\n",
        "types = 0\n",
        "tokens = 0\n",
        "\n",
        "# YOUR CODE HERE! POPULATE THE types AND tokens VARIABLES WITH THE CORRECT VALUES!\n",
        "para = paragraph.split() # By default, it splits based on whitespace\n",
        "types = len(set(para))\n",
        "tokens = len(para)\n",
        "\n",
        "\n",
        "# DO NOT MODIFY THE BELOW LINE!\n",
        "print('Number of word types: %d, number of word tokens:%d' % (types, tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of word types: 76, number of word tokens:100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5f5YpclYjbh"
      },
      "source": [
        "##Question 2.2 (5 points)\n",
        "Now let's look at the most frequently used word **types** in this paragraph. Write some code in the below cell to print out the ten most frequently-occurring types. We have initialized a [Counter](https://docs.python.org/2/library/collections.html#collections.Counter) object that you should use for this purpose. In general, Counters are very useful for text processing in Python.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpjx2fGbh_tp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "013f9198-08dd-4753-9883-4ba4ecc06082"
      },
      "source": [
        "from collections import Counter\n",
        "para = paragraph.split() # By default, it splits based on whitespace \n",
        "c = Counter(para)\n",
        "# DO NOT MODIFY THE BELOW LINES!\n",
        "for word, count in c.most_common()[:10]:\n",
        "    print(word, count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i 6\n",
            "the 6\n",
            "to 4\n",
            "a 3\n",
            "and 3\n",
            "my 3\n",
            "it 2\n",
            "that 2\n",
            "was 2\n",
            "with 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQfA98xLjIv9"
      },
      "source": [
        "##Question 2.3 (5 points)\n",
        "What do you notice about these words and their linguistic functions (i.e., parts-of-speech)? These words are known as \"stopwords\" in NLP and are often removed from the text before any computational modeling is done. Why do you think that is?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jj042SWkFvP"
      },
      "source": [
        "*   In terms of linguistic functions i.e. parts of speech, these words mostly belong to determiners, preposition, pronouns or conjunctions. When we are performing any bag of words based method/ computational modeling, they are so generic that they don't add much value or drive our analysis and removing them gives us the benefit of having a reduced dimensionality. Hence, these are often removed.\n",
        "*   However, one should be careful with the removal of stopwords where capturing context is involved. e.g. Negation (not, never etc.) might completely alter the context of the entire sentence.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9OxWy-CYlp4"
      },
      "source": [
        "##Question 3.1 (10 points)\n",
        "In *neural* language models, we represent words with low-dimensional vectors also called *embeddings*. We use these embeddings to compute a vector representation $\\boldsymbol{x}$ of a given prefix, and then predict the probability of the next word conditioned on $\\boldsymbol{x}$. In the below cell, we use [PyTorch](https://pytorch.org), a machine learning framework, to explore this setup. We provide embeddings for the prefix \"Alice talked to\"; your job is to combine them into a single vector representation $\\boldsymbol{x}$ using [element-wise vector addition](http://www.glue.umd.edu/afs/glue.umd.edu/system/info/olh/Numerical/Matlab_Matrix_Manipulation_Software/Matrix_Vector_Operations/elementwise). \n",
        "\n",
        "*TIP: if you're finding the PyTorch coding problems difficult, you may want to run through [the 60 minutes blitz tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)!*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Su_j1JY1QG5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cd5d51d-b91e-4474-9c92-18bc821e1115"
      },
      "source": [
        "import torch\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "prefix = 'Alice talked to'\n",
        "\n",
        "# spend some time understanding this code / reading relevant documentation! \n",
        "# this is a toy problem with a 5 word vocabulary and 10-d embeddings\n",
        "embeddings = torch.nn.Embedding(num_embeddings=5, embedding_dim=10)\n",
        "vocab = {'Alice':0, 'talked':1, 'to':2, 'Bob':3, '.':4}\n",
        "\n",
        "# we need to encode our prefix as integer indices (not words) that index \n",
        "# into the embeddings matrix. the below line accomplishes this.\n",
        "# note that PyTorch inputs are always Tensor objects, so we need\n",
        "# to create a LongTensor out of our list of indices first.\n",
        "indices = torch.LongTensor([vocab[w] for w in prefix.split()])\n",
        "prefix_embs = embeddings(indices)\n",
        "print('prefix embedding tensor size: ', prefix_embs.size())\n",
        "\n",
        "# okay! we now have three embeddings corresponding to each of the three\n",
        "# words in the prefix. write some code that adds them element-wise to obtain\n",
        "# a representation of the prefix! store your answer in a variable named \"x\".\n",
        "\n",
        "### YOUR CODE HERE!\n",
        "x = torch.sum(prefix_embs,dim=0)\n",
        "### DO NOT MODIFY THE BELOW LINE\n",
        "print('embedding sum: ', x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prefix embedding tensor size:  torch.Size([3, 10])\n",
            "embedding sum:  tensor([-0.1770, -2.3993, -0.4721,  2.6568,  2.7157, -0.1408, -1.8421, -3.6277,\n",
            "         2.2783,  1.1165], grad_fn=<SumBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F41LYDeoZPYI"
      },
      "source": [
        "##Question 3.2 (5 points)\n",
        "Modern language models do not use element-wise addition to combine the different word embeddings in the prefix into a single representation (a process called *composition*). What is a major issue with element-wise functions that makes them unsuitable for use as composition functions? \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MaDZekXs0C7"
      },
      "source": [
        "Composition (element wise addition to combine different word embeddings on prefix to form a single representation) is not used because:\n",
        "1. It does not take into account the order of representation of the words. Even if the words were jumbled in a random order, the final output embedding after composition would be same which might not be very helpful\n",
        "2.   There could be different combination of words with entirely different meaning which when averaging might result in same final output embeddings. However, this problem is not very much prominent in high dimensional embedding space. (e.g. In a hypothetical scenario, average of (-1,0,1) is same as average of (0,0,0) in a single dimensional space where (x,y,z) are the single dimensional vector (represented by integers here) representation of any three words respectively)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-xGz2_cZVs7"
      },
      "source": [
        "##Question 3.3 (10 points)\n",
        "One very important function in neural language models (and for basically every task we'll look at this semester) is the [softmax](https://pytorch.org/docs/master/nn.functional.html#softmax), which is defined over an $n$-dimensional vector $<x_1, x_2, \\dots, x_n>$ as $\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{1 \\leq j \\leq n} e^{x_j}}$. Let's say we have our prefix representation $\\boldsymbol{x}$ from before. We can use the softmax function, along with a linear projection using a matrix $W$, to go from $\\boldsymbol{x}$ to a probability distribution $p$ over the next word: $p = \\text{softmax}(W\\boldsymbol{x}). $Let's explore this in the code cell below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mClmHIeowL4V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0334403-fce4-4943-bf23-b25bd3e8b10d"
      },
      "source": [
        "# remember, our goal is to produce a probability distribution over the \n",
        "# next word, conditioned on the prefix representation x. This distribution\n",
        "# is thus over the entire vocabulary (i.e., it is a 5-dimensional vector).\n",
        "# take a look at the dimensionality of x, and you'll notice that it is a \n",
        "# 10-dimensional vector. first, we need to **project** this representation\n",
        "# down to 5-d. We'll do this using the below matrix:\n",
        "W = torch.rand(10, 5)\n",
        "# use this matrix to project x to a 5-d space, and then\n",
        "# use the softmax function to convert it to a probability distribution.\n",
        "# this will involve using PyTorch to compute a matrix/vector product.\n",
        "# look through the documentation if you're confused (torch.nn.functional.softmax)\n",
        "\n",
        "### YOUR CODE HERE\n",
        "projected = torch.matmul(W.T,x)\n",
        "probs = torch.nn.functional.softmax(projected)\n",
        "\n",
        "\n",
        "### DO NOT MODIFY THE BELOW LINE!\n",
        "print('probability distribution', probs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability distribution tensor([0.0718, 0.0998, 0.1331, 0.6762, 0.0191], grad_fn=<SoftmaxBackward>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOlrqnSqZ3H8"
      },
      "source": [
        "##Question 3.4 (15 points)\n",
        "So far, we have looked at just a single prefix (\"Alice talked to\"). In practice, it is common for us to compute many prefixes in one computation, as this enables us to take advantage of GPU parallelism and also obtain better gradient approximations (we'll talk more about the latter point later). This is called *batching*, where each prefix is an example in a larger batch. Here, you'll redo the computations from the previous cells, but instead of having one prefix, you'll have a batch of two prefixes. The final output of this cell should be a 2x5 matrix that contains two probability distributions, one for each prefix. **NOTE: YOU WILL LOSE POINTS IF YOU USE ANY LOOPS IN YOUR ANSWER!** Your code should be completely vectorized (a few large computations is faster than many smaller ones)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZarWwkESM7-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e859d86d-e2ef-4562-efc9-a1a58fa53373"
      },
      "source": [
        "# For this problem, we'll just copy our old prefix over three times\n",
        "# to form a batch. in practice, each example in the batch would be different.\n",
        "batch_indices = torch.cat(2 * [indices]).reshape((2, 3))\n",
        "batch_embs = embeddings(batch_indices)\n",
        "print('batch embedding tensor size: ', batch_embs.size())\n",
        "\n",
        "# now, follow the same procedure as before:\n",
        "# step 1: compose each example's embeddings into a single representation\n",
        "# using element-wise addition. HINT: check out the \"dim\" argument of the torch.sum function!\n",
        "\n",
        "# step 2: project each composed representation into a 5-d space using matrix W\n",
        "# step 3: use the softmax function to obtain a 2x5 matrix with the probability distributions\n",
        "\n",
        "# please store this probability matrix in the \"batch_probs\" variable.\n",
        "# Get sum in batch matrix\n",
        "emb_rep     = torch.sum(batch_embs,dim=1)\n",
        "# Initialize a weights vector (Same as in above question)\n",
        "W           = torch.rand(10, 5)\n",
        "# Take projections\n",
        "projected   = torch.matmul(emb_rep,W)\n",
        "# Generate Softmax probabilities\n",
        "batch_probs = torch.nn.functional.softmax(projected,dim=1)\n",
        "### DO NOT MODIFY THE BELOW LINE\n",
        "print(\"batch probability distributions:\", batch_probs)\n",
        "\n",
        "# Reference \n",
        "# https://stackoverflow.com/questions/37142135/sum-numpy-ndarray-with-3d-array-along-a-given-axis-1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch embedding tensor size:  torch.Size([2, 3, 10])\n",
            "batch probability distributions: tensor([[0.0655, 0.0028, 0.0023, 0.1176, 0.8118],\n",
            "        [0.0655, 0.0028, 0.0023, 0.1176, 0.8118]], grad_fn=<SoftmaxBackward>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTxHxdG5BpQU"
      },
      "source": [
        "## Question 4 (20 points)\n",
        "\n",
        "Choose  one  paper  from  either [ACL 2021](https://aclanthology.org/events/acl-2021/#2021-acl-long) or [NAACL 2021](https://aclanthology.org/volumes/2021.naacl-main/) that you find interesting. A good way to do this is by scanning the titles and abstracts; there are hundreds of papers so take your time before selecting one!  Then, write a summary in  your own words of the paper you chose. Your summary should answer the following questions: what is its motivation? Why should anyone care about it? Were there things in the paper that you didn't understand at all? What were they? Fill out the below cell, and make sure to write 2-4 paragraphs for the summary to receive full credit! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpg8eU9wNBci"
      },
      "source": [
        "**Title of paper**: \"Enhancing Content Preservation in Text Style Transfer\n",
        "Using Reverse Attention and Conditional Layer Normalization\"\n",
        "\n",
        "**Authors**: Dongkyu Lee, Zhiliang Tian, Lanqing Xue, Nevin L. Zhang\n",
        "\n",
        "**Conference name**: ACL & 11th IJCNLP (Volume 1: Long Papers)\n",
        "\n",
        "**URL**: https://aclanthology.org/2021.acl-long.8.pdf\n",
        "\n",
        "**Your summary**: \\\n",
        "Motivation and why should we care about this: In most of the style transfer tasks in the textual data, it happens that researchers focus to filter out the style information by removing tokens that they believe contains style using different techniques. But this is based on a huge assumption that a token that contains style information would never contain the content information. In fact, both content and style information can be present together in the token and removal of a style token may result in a big content loss as well. Hence the author argue that their work addresses this problem by  by two important contributions:\n",
        "\n",
        "1.   Implicit removal of style information of each token with reverse attention \n",
        "2.   Taking inspiration from of layer normalization in Computer Vision based style transfer, they also utilize \"Conditional Layer Normalization\"\n",
        "\n",
        "Content preservation is very important otherwise the object of style transfer would be a futile effort. Hence, authors have explored how to solve this problem.\n",
        "\n",
        "They call it RACoLN (Reverse Attention and Conditional Layer Normalization).\n",
        "\n",
        "Summary: For the first contribution described above, authors assume that the the tokens are composed of both style and content and representation of each in a given token may vary. So, they compute a attention score that they assume represents the style information using a layer of bidirectional GRU and compute the reverse attention by subtracting attention scores (softmax probabilities) from 1. Then they use this as weights to reconstruct the sentence. In this way, the final concatenated sentence tokens are only left with content information. These sentences are then passed through a content normalization layer. In this way the content representation is dynamic. This generates content dependent style representation and is concatenated with output from previous step (style independent content representation) before passing through a GRU layer. This GRU layer helps in generating the final new stylized representation of the sentence. In short, the entire architecture consisted of four major modules: Style marker module, Style Removal module, Stylizer (Content Normalization Layer) and the Decoder module.\n",
        "\n",
        "In order to perform training, they combine four different losses: \n",
        "\n",
        "1.   Self Reconstruction Loss: Idea was that the trained modules should be able to reproduce the same style when same target style is used on only the content based sentence\n",
        "2.   Cycle Reconstruction Loss: Idea was that if we transfer sequence x into another style and then back to original style, we should be able to reconstruct input x\n",
        "3.   Content Loss: Since the assumption is we retain content throughout, the content loss should be negligible\n",
        "4.   Style Transfer Loss: Idea is that we should be able to transfer style effectively \n",
        "\n",
        "They combine the four loss functions weighted by lambda parameters. In terms of evaluation, they infer that for the content preservation part, they were able to surpass previous SOTA (state of the art) by a margin of more than 4 points BLEU score on Yelp dataset. Apart from this, the authors compare their performance with previous SOTA methods on other metrics like perplexity and BERT score (for computing contextual similarity). They also perform human evaluations.\n",
        "\n",
        "One thing to note that in the present paper, the authors have utilized positive and negative sentiments as representation of two style forms and it would be interesting to see how the entire approach performs on a multi-style representation. Towards the end of the paper, authors show how the t-SNE representation of style and content looks for the Yelp dataset.  \n",
        "\n",
        "Things that I did not understand: The paper mentions about the use of \"soft-sampling\" and \"gumbel-softmax\" in the decoder. They say that as the greedy decoding process did not allow gradient flow (methods being non-differentiable), they had to find a workaround. They say that for the smooth gradient flow, product of probability distribution of each time step is required with weight of the embedding layer. I did not have much idea about the soft-sampling terminology and had to google to understand the working this soft-sampling approach. Also, the step where they explain blocking of gradient update was initially not very clear to me.\n",
        "\n",
        "Overall, the paper is well structured and presents it's case cogently. It does seem that it was able to achieve its objective of content preservation while at the same time effectively transferring the style.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSBn7xu5FuMi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWZT8JJJFuGB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6SVrSzgFuCj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}