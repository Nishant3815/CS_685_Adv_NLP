{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nishant_CS685_HW1_annotation_task_final.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SgNZTjrhcHa0"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgNZTjrhcHa0"
      },
      "source": [
        "## Homework 1, CS685 Fall 2021\n",
        "\n",
        "### This is due on November 5th, 2021. This notebook is to be submitted via Gradescope as a PDF file, while your three dataset files (annotator1.csv, annotator2.csv, and final_data.csv) should be emailed to cs685instructors@gmail.com with the subject line formatted as **Firstname_Lastname_HW1data**. 100 points total.\n",
        "\n",
        "#### IMPORTANT: After copying this notebook to your Google Drive, please paste a link to it below. To get a publicly-accessible link, hit the *Share* button at the top right, then click \"Get shareable link\" and copy over the result. If you fail to do this, you will receive no credit for this homework!\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##### *How to submit this problem set:*\n",
        "- Write all the answers in this Colab notebook. Once you are finished, generate a PDF via (File -> Print -> Save as PDF) and upload it to Gradescope.\n",
        "  \n",
        "- **Important:** check your PDF before you submit to Gradescope to make sure it exported correctly. If Colab gets confused about your syntax, it will sometimes terminate the PDF creation routine early.\n",
        "\n",
        "- **Important:** on Gradescope, please make sure that you tag each page with the corresponding question(s). This makes it significantly easier for our graders to grade submissions, especially with the long outputs of many of these cells. We will take off points for submissions that are not tagged.\n",
        "\n",
        "- When creating your final version of the PDF to hand in, please do a fresh restart and execute every cell in order. One handy way to do this is by clicking `Runtime -> Run All` in the notebook menu.\n",
        "\n",
        "---\n",
        "\n",
        "##### *Academic honesty*\n",
        "\n",
        "- We will audit the Colab notebooks from a set number of students, chosen at random. The audits will check that the code you wrote actually generates the answers in your PDF. If you turn in correct answers on your PDF without code that actually generates those answers, we will consider this a serious case of cheating. See the course page for honesty policies.\n",
        "\n",
        "- We will also run automatic checks of Colab notebooks for plagiarism. Copying code from others is also considered a serious case of cheating.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2dYC4HbZL-j"
      },
      "source": [
        "# Part 1: Annotation\n",
        "\n",
        "In this homework, you will first collect a labeled dataset of **120** sentences for a text classification task of your choice. This process will include:\n",
        "\n",
        "1. *Data collection*: Collect 120 sentences from any source you find interesting (e.g., literature, Tweets, news articles, reviews, etc.)\n",
        "\n",
        "2. *Task design*: Come up with a binary (i.e., only two labels) sentence-level classification task that you would like to perform on your sentences. Don't choose something boring like sentiment analysis... Be creative! Write up annotator guidelines/instructions on how you would like people to label your data.\n",
        "\n",
        "3. On your dataset, collect annotations from **two** classmates for your task. Everyone in this class will need to both create their own dataset and also serve as an annotator for two other classmates. In order to get everything done on time, you need to complete the following steps:\n",
        "\n",
        "> *   Find two classmates willing to label 120 sentences each (we will open a Piazza thread to help facilitate this).\n",
        "*   Send them your annotation guidelines and a way that they can easily annotate the data (e.g., a spreadsheet or Google form)\n",
        "*   Collect the labeled data from each of the two annotators.\n",
        "*   Sanity check the data for basic cleanliness (are all examples annotated? are all labels allowable ones?)\n",
        "\n",
        "4. Collect feedback from annotators about the task including annotation time and obstacles encountered (e.g., maybe your guidelines were confusing! or maybe some sentences were particularly hard to annotate!)\n",
        "\n",
        "5. Calculate and report inter-annotator agreement.\n",
        "\n",
        "6. Aggregate output from both annotators to create final dataset.\n",
        "\n",
        "7. Perform NLP experiments on your new dataset!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Heui1z3IjZh_"
      },
      "source": [
        "## Question 1.1 (10 points):\n",
        "Describe the source of your unlabeled data, why you chose it, and what kind of sentence selection process you used (if any) to choose 120 sentences for annotation. Also briefly describe the text classification task that you will be collecting labels for in the next section.\n",
        "\n",
        "####*SOURCE OF DATA: PERSONAL E-MAIL ACCOUNT*####\n",
        "####*WHY CHOOSE IT: The goal is to create a “Personalization Mail Assistant” for myself. Many a times, I face trouble while finding a useful mail. Thus, I am trying to create something that I mark as important/relevant for me so that I can quickly go back to these mails whenever required. Goal is also to evaluate and understand how well a transformer model will be able to perform transfer learning using such highly personalized instances unlike normal tasks like sentiment analysis, textual entailment etc.*####\n",
        "####*SENTENCE SELECTION PROCESS: The sentence selection process was from mail. For relevant sentences, I decided to choose already starred mails from my previous experience and for the other class, I chose sentences from fields like social media mails, promotions, non-relevant mails. Also, while choosing the sentences, I chose close to 40% of positive class instances and 60% negative class instances so that model sees close to equal numbers in these categories but kept number for positive class a bit low to mimick real life scenario*####\n",
        "####*TASK DESCRIPTION for TEXT CLASSIFICATION: The task is to create a “Personalization Mail Assistant”for myself. There are two labels: 1(Starrable/Important Mails) i.e. mail contents that might be very relevant and personal for me and 0(Non-Starrable/Non-Relevant Mails) which include categories which are not personally relevant for me. So, the idea is when a mail enters my inbox, I can use this model to immediatelely categorize into personally important vs non-important so that I can refer back to it in future quickly*####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EynpTLydj7IM"
      },
      "source": [
        "## Question 1.2 (25 points):\n",
        "Copy the annotation guidelines that you provided to your classmates below. We expect that these guidelines will be very detailed (and as such could be fairly long). You must include:\n",
        "\n",
        "> *   The two categories for your binary classification problem, including the exact strings you want annotators to use while labeling. \n",
        "*   Descriptions of the categories and what they mean.\n",
        "*   Representative examples of each category (i.e., sentences from outside your dataset that you have manually labeled to give annotators an idea of how to perform the task)\n",
        "*   A discussion of of tricky corner cases, and criteria to help the annotator decide them. If you look at the data and think about how an annotator could do the task, you will likely find a bunch of these!\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## ANNOTATION GUIDELINES##\n",
        "####GENERAL TASK INTRODUCTION: The task is to create a “Personalization Mail Assistant”. Think of this as a “label:starred” feature in GMail. But having a more personal touch as I am trying to create something that I mark as important/relevant for me so that I can quickly go back to these mails whenever required. Goal is also to evaluate how well a transformer model will be to perform transfer learning using such highly personalized instances.####\n",
        "\n",
        "####CATEGORIES: There are two labels: 1(Starrable/Relevant Mails) i.e. mail contents that might be very relevant and personal for me and 0(Non-Starrable/Non-Relevant Mails).####\n",
        "\n",
        "####CATEGORY DESCRIPTION: ####\n",
        "##### FOR 1(Starrable/Relevant Mails): \n",
        "*   Information that is personal in nature for a student or that of a prospective employee are starrable and important (like offer letter, online assessment information, interview information, rejection information, acceptance information)\n",
        "*   Information that mention about an important attachment in the mail, a zoom/meeting link in the mail, card information fall in this category\n",
        "* Information from colleges, CICS, conferences and publications and other similar institutions are starrable\n",
        "* Important registrations like flights, conferences etc.\n",
        "* Information between faculty and students are starrable\n",
        "\n",
        "\n",
        "##### FOR 0(Non-Starrable/Non-Relevant Mails): \n",
        "*   Promotional Mails having offers are non-starrable (like 10% discount, visit shop, 300 off etc.)\n",
        "*   Abstract sentences like quotes are definitely non-starrable\n",
        "*   Notifications from social media like (birthday and friend/LinkedIn request) are non-starrable \n",
        "*  Random sentences that have no significance for a student or feels like a part of a paragraph from somewhere also falls into this category (e.g. Having a detailed financial & business plan before approaching investors helps entrepreneurs in getting the right kind of funding.)\n",
        "\n",
        "#### EXAMPLES####\n",
        "##### FOR 1(Starrable/Relevant Mails): \n",
        "*   We will let you know when our 2022 roles start to be posted in the coming month or so to our University Careers Page. We look forward to seeing your applications.\n",
        "*  We are asking for your help in holding additional office hours and answering piazza posts, if that fits within your time commitment.\n",
        "The students have asked many questions on piazza and the TAs are quite overwhelmed.\n",
        "* Your flight ticket for Bangalore-Patna-Bangalore is confirmed\n",
        "\n",
        "##### FOR 0(Non-Starrable/Non-Relevant Mails): \n",
        "*   Whether you’re decorating, shopping for a gift, or looking to replace that faulty blender, everything you need from Bed Bath & Beyond is now easily available on DoorDash.\n",
        "*   Whether you’re decorating, shopping for a gift, or looking to replace that faulty blender, everything you need from Bed Bath & Beyond is now easily available on DoorDash.\n",
        "* Ends Friday: 30% off your pass to reconnecting with the data science community\n",
        "\n",
        "\n",
        "#### CORNER CASES: In cases of confusion, I would suggest to re-read the sentence and then ask yourself a question: as a student or an employee do you think this information can be relevant at personal level. Also, if it is promotional or social media related content, feel free to directly flag it as non-starrable (0). I have added one more Column: “Not-Confident”. In case still, you feel that you are not confident enough, select the best category and set this flag as 1. I will leverage this flag for conflict resolution or determining the best label from the two labellers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "935duWappc-o"
      },
      "source": [
        "## Question 1.3 (5 points):\n",
        "Write down the names and emails of the two classmates who will be annotating your data below. Once they are finished annotating, create two .csv files (annotator1.csv and annotator2.csv) that contains each annotator's labels. The file should have two columns with headers **text** and **label**, respectively. You will include these files in an email to the instructors account when you're finished with this homework. \n",
        "\n",
        "*The tweets.csv file provided as an example in Part 2 below uses the same format.*\n",
        "\n",
        "### *Annotator 1: Abhishek Lalwani (alalwani@umass.edu)* ###\n",
        "### *Annotator 2: Prahlad Das (prahladdas@umass.edu)* ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcIMegO_uPRK"
      },
      "source": [
        "## Question 1.4 (10 points):\n",
        "After both annotators have finished labeling the 120 sentences you gave them, ask them for feedback about your task and the provided annotation guidelines. If you were to collect more labeled data for this task in the future, what would you change from your current setup? Why? Please include a summary of annotator feedback (with specific examples that they found challenging to label) in your answer.\n",
        "\n",
        "\n",
        "### What would you change from current setup and why ? ###\n",
        "\n",
        "*   If I were to change something in the current setup, I would reselect those examples which I feel are harder for model to classify correctly by the model. Reasoning: This way we can ensure that the model actually learns the cases where the differentiation is not very significant (instances close to the decision boundary) instead of just classifying simple instances. \n",
        "\n",
        "### Feedback Summary From Prahlad Das ###\n",
        "*   It was good to annotate the data as I could relate all of them with my emails. Though there were a few tricky cases where it was difficult to decide like ' We connect you with 3 volunteers and 4 peers who will help you find internships.' and ' Covering topics such as LSTMs, generative adversarial transformers, embeddings, and more, this is some standout NLP research from 2021 so far. ' I was not sure if they were spam or requested services as no other information was available.\n",
        "Annotation guideline was clear with description of both categories with nice examples. Even how to resolve corner cases was explained clearly.\n",
        "\n",
        "### Feedback Summary from Abhishek Lalwani ###\n",
        "*   Annotation guidelines were very clear and most of the selected examples were meticulously chosen. Only in some cases, I felt that it would have been better if there were more than two categories for annotation but given binary nature of the task, it was understandable. One of the example sentence that I found confusing was: \"Covering topics such as LSTMs, generative adversarial transformers, embeddings, and more, this is some standout NLP research from 2021 so far.\" as it was difficult to determine which class this belonged to without much context\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ltte0Z-vD0u"
      },
      "source": [
        "## Question 1.5 (10 points):\n",
        "Now, compute the inter-annotator agreement between your two annotators. Upload both .csv files to your Colab session (click the folder icon in the sidebar to the left of the screen). In the code cell below, read the data from the two files and compute both the raw agreement (% of examples for which both annotators agreed on the label) and the [Cohen's Kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa). Feel free to use implementations in existing libraries (e.g., [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html)). After you're done, paste the numbers in the text cell that follows your code. \n",
        "\n",
        "*If you're curious, Cohen suggested the Kappa result be interpreted as follows: values ≤ 0 as indicating no agreement and 0.01–0.20 as none to slight, 0.21–0.40 as fair, 0.41– 0.60 as moderate, 0.61–0.80 as substantial, and 0.81–1.00 as almost perfect agreement.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD7yvkwgy82S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9540d20c-4a17-4181-f04b-c2f0a27d61ed"
      },
      "source": [
        "### WRITE CODE TO LOAD ANNOTATIONS AND \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "data1 = pd.read_csv(\"annotator1.csv\")\n",
        "data2 = pd.read_csv(\"annotator2.csv\")\n",
        "label1 = data1['label'].tolist()\n",
        "label2 = data2['label'].tolist()\n",
        "data1['label'] = data1['label'].astype('int32')\n",
        "data2['label'] = data2['label'].astype('int32')\n",
        "### COMPUTE AGREEMENT + COHEN'S KAPPA HERE!\n",
        "suma = 0\n",
        "for i in range(len(label1)):\n",
        "  if label1[i]==label2[i]:\n",
        "    suma+=1\n",
        "raw_agreement = suma*100/len(label1)\n",
        "ck_score      = cohen_kappa_score(data1['label'],data2['label'])\n",
        "print(\"Raw Agreement is: \", raw_agreement)\n",
        "print(\"Cohen Kappa Score is: \", ck_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Agreement is:  90.0\n",
            "Cohen Kappa Score is:  0.7869192068659367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd7Xq4SKzF5U"
      },
      "source": [
        "###*RAW AGREEMENT*: 90.0%\n",
        "###*COHEN'S KAPPA*: 0.786"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2-1LkRHze5N"
      },
      "source": [
        "## Question 1.6 (10 points):\n",
        "To form your final dataset, you need to *aggregate* the annotations from both annotators (i.e., for cases where they disagree, you need to choose a single label). Use any method you like other than random label selection to perform this aggregation (e.g., have the two annotators discuss each disagreement and come to consensus, or choose the label you agree with the most). Upload your final dataset to the Colab session (in the same format as the other two files) as final_dataset.csv. Remember to include this file in your final email to us!\n",
        "\n",
        "### *I had asked where the two annotators were not confident in the annotation sheet that they had shared with me. First I used that for the resolution and selected the one that I thought to be best with my  and more confident annotator's agreement. For other remaining ones, I chose the one that I had already starred in my e-mail as they were the true labels in my case (i.e. my perosonal best agreement)* ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d23zfO_ALKeB"
      },
      "source": [
        "# Part 2: Text classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N25dvF4jvYoy"
      },
      "source": [
        "Now we'll move onto fine-tuning  pretrained language models specifically on your dataset. This part of the homework is meant to be an introduction to the HuggingFace library, and it contains code that will potentially be useful for your final projects. Since we're dealing with large models, the first step is to change to a GPU runtime.\n",
        "\n",
        "## Adding a hardware accelerator\n",
        "\n",
        "Please go to the menu and add a GPU as follows:\n",
        "\n",
        "`Edit > Notebook Settings > Hardware accelerator > (GPU)`\n",
        "\n",
        "Run the following cell to confirm that the GPU is detected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edOh9ooiIW1B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb9b6618-4479-4032-bc9b-d7ba3aa477dd"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Confirm that the GPU is detected\n",
        "\n",
        "assert torch.cuda.is_available()\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = torch.cuda.get_device_name()\n",
        "n_gpu = torch.cuda.device_count()\n",
        "print(f\"Found device: {device_name}, n_gpu: {n_gpu}\")\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found device: Tesla K80, n_gpu: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrvH7xx9LnMC"
      },
      "source": [
        "## Installing Hugging Face's Transformers library\n",
        "We will use Hugging Face's Transformers (https://github.com/huggingface/transformers), an open-source library that provides general-purpose architectures for natural language understanding and generation with a collection of various pretrained models made by the NLP community. This library will allow us to easily use pretrained models like `BERT` and perform experiments on top of them. We can use these models to solve downstream target tasks, such as text classification, question answering, and sequence labeling.\n",
        "\n",
        "Run the following cell to install Hugging Face's Transformers library and download a sample data file called tweets.csv that contains tweets about airlines along with a negative, neutral, or positive sentiment rating. Note that you will be asked to link with your Google Drive account to download some of these files. If you're concerned about security risks (there have not been any issues in previous semesters), feel free to make a new Google account and use it for this homework!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtqS2e5fxpqa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfb86d5a-d1b9-48db-aec2-1877cb65becf"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "print('success!')\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Download helper functions file\n",
        "helper_file = drive.CreateFile({'id': '16HW-z9Y1tM3gZ_vFpJAuwUDohz91Aac-'})\n",
        "helper_file.GetContentFile('helpers.py')\n",
        "print('helper file downloaded! (helpers.py)')\n",
        "\n",
        "# Download sample file of tweets\n",
        "data_file = drive.CreateFile({'id': '1QcoAmjOYRtsMX7njjQTYooIbJHPc6Ese'})\n",
        "data_file.GetContentFile('tweets.csv')\n",
        "print('sample tweets downloaded! (tweets.csv)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "success!\n",
            "helper file downloaded! (helpers.py)\n",
            "sample tweets downloaded! (tweets.csv)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8XIL7wPovVX"
      },
      "source": [
        "The cell below imports some helper functions we wrote to demonstrate the task on the sample tweet dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Taseb33Sovg0"
      },
      "source": [
        "from helpers import tokenize_and_format, flat_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKc0xYh-MAbc"
      },
      "source": [
        "# Part 1: Data Prep and Model Specifications\n",
        "\n",
        "Upload your data using the file explorer to the left. We have provided a function below to tokenize and format your data as BERT requires. Make sure that your csv file, titled final_data.csv, has one column \"text\" and another column \"labels\" containing integers.\n",
        "\n",
        "If you run the cell below without modifications, it will run on the tweets.csv example data we have provided. It imports some helper functions we wrote to demonstrate the task on the sample tweet dataset. You should first run all of the following cells with tweets.csv just to see how everything works. Then, once you understand the whole preprocessing / fine-tuning process, change the csv in the below cell to your final_data.csv file, add any extra preprocessing code you wish, and then run the cells again on your own data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGhkeLQlNNr8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53b037c6-fadd-4751-d571-cd95ac29b9fb"
      },
      "source": [
        "from helpers import tokenize_and_format, flat_accuracy\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('final_data.csv')\n",
        "#df = pd.read_csv('tweets.csv')\n",
        "\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "texts = df.text.values\n",
        "labels = df.label.values\n",
        "\n",
        "### tokenize_and_format() is a helper function provided in helpers.py ###\n",
        "input_ids, attention_masks = tokenize_and_format(texts)\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', texts[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  As a result of this particular day, we welcomed approximately 40,000 new retail colleagues, including pharmacists, store associates, and pharmacy technicians!\n",
            "Token IDs: tensor([  101,  2004,  1037,  2765,  1997,  2023,  3327,  2154,  1010,  2057,\n",
            "        10979,  3155,  2871,  1010,  2199,  2047,  7027,  8628,  1010,  2164,\n",
            "         6887, 27292,  6305,  5130,  1010,  3573,  9228,  1010,  1998, 13882,\n",
            "        20202,   999,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3D-CzQEUXYz"
      },
      "source": [
        "## Create train/test/validation splits\n",
        "\n",
        "Here we split your dataset into 3 parts: a training set, a validation set, and a testing set. Each item in your dataset will be a 3-tuple containing an input_id tensor, an attention_mask tensor, and a label tensor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGgeZ3M0UWs0"
      },
      "source": [
        "total = len(df)\n",
        "\n",
        "num_train = int(total * .8)\n",
        "num_val = int(total * .1)\n",
        "num_test = total - num_train - num_val\n",
        "\n",
        "# make lists of 3-tuples (already shuffled the dataframe in cell above)\n",
        "\n",
        "train_set = [(input_ids[i], attention_masks[i], labels[i]) for i in range(num_train)]\n",
        "val_set = [(input_ids[i], attention_masks[i], labels[i]) for i in range(num_train, num_val+num_train)]\n",
        "test_set = [(input_ids[i], attention_masks[i], labels[i]) for i in range(num_val + num_train, total)]\n",
        "\n",
        "train_text = [texts[i] for i in range(num_train)]\n",
        "val_text = [texts[i] for i in range(num_train, num_val+num_train)]\n",
        "test_text = [texts[i] for i in range(num_val + num_train, total)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCr006iTkqwM"
      },
      "source": [
        "Here we choose the model we want to finetune from https://huggingface.co/transformers/pretrained_models.html. Because the task requires us to label sentences, we wil be using BertForSequenceClassification below. You may see a warning that states that `some weights of the model checkpoint at [model name] were not used when initializing. . .` This warning is expected and means that you should fine-tune your pre-trained model before using it on your downstream task. See [here](https://github.com/huggingface/transformers/issues/5421#issuecomment-652582854) for more info."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPo640_ZlEPK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f53d9b8d-c6f2-46e2-fd6b-c6b8455d9a9f"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3lLdoW_le3M"
      },
      "source": [
        "# ACTION REQUIRED #\n",
        "\n",
        "Define your fine-tuning hyperparameters in the cell below (we have randomly picked some values to start with). We want you to experiment with different configurations to find the one that works best (i.e., highest accuracy) on your validation set. Feel free to also change pretrained models to others available in the HuggingFace library (you'll have to modify the cell above to do this). You might find papers on BERT fine-tuning stability (e.g., [Mosbach et al., ICLR 2021](https://openreview.net/pdf?id=nzpLWnVAyah)) to be of interest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd2JdC6IletV"
      },
      "source": [
        "batch_size = 32\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 8e-6, # args.learning_rate - default is 5e-5\n",
        "                  eps = 1e-7 # args.adam_epsilon  - default is 1e-8\n",
        "                )\n",
        "epochs = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd4fwn_el1ge"
      },
      "source": [
        "# Fine-tune your model\n",
        "Here we provide code for fine-tuning your model, monitoring the loss, and checking your validation accuracy. Rerun both of the below cells when you change your hyperparameters above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_Mzr-kd5RaY"
      },
      "source": [
        "import numpy as np\n",
        "# function to get validation accuracy\n",
        "def get_validation_performance(val_set):\n",
        "    # Put the model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "\n",
        "    num_batches = int(len(val_set)/batch_size) + 1\n",
        "\n",
        "    total_correct = 0\n",
        "\n",
        "    for i in range(num_batches):\n",
        "\n",
        "      end_index = min(batch_size * (i+1), len(val_set))\n",
        "\n",
        "      batch = val_set[i*batch_size:end_index]\n",
        "      \n",
        "      if len(batch) == 0: continue\n",
        "\n",
        "      input_id_tensors = torch.stack([data[0] for data in batch])\n",
        "      input_mask_tensors = torch.stack([data[1] for data in batch])\n",
        "      label_tensors = torch.stack([data[2] for data in batch])\n",
        "      \n",
        "      # Move tensors to the GPU\n",
        "      b_input_ids = input_id_tensors.to(device)\n",
        "      b_input_mask = input_mask_tensors.to(device)\n",
        "      b_labels = label_tensors.to(device)\n",
        "        \n",
        "      # Tell pytorch not to bother with constructing the compute graph during\n",
        "      # the forward pass, since this is only needed for backprop (training).\n",
        "      with torch.no_grad():        \n",
        "\n",
        "        # Forward pass, calculate logit predictions.\n",
        "        outputs = model(b_input_ids, \n",
        "                                token_type_ids=None, \n",
        "                                attention_mask=b_input_mask,\n",
        "                                labels=b_labels)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "        \n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the number of correctly labeled examples in batch\n",
        "        pred_flat = np.argmax(logits, axis=1).flatten()\n",
        "        labels_flat = label_ids.flatten()\n",
        "        num_correct = np.sum(pred_flat == labels_flat)\n",
        "        total_correct += num_correct\n",
        "        \n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_correct / len(val_set)\n",
        "    return avg_val_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTf_ipbjWNoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a695f48-f16b-4fb1-810d-ad54492f6e8a"
      },
      "source": [
        "import random\n",
        "\n",
        "# training loop\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode.\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    num_batches = int(len(train_set)/batch_size) + 1\n",
        "\n",
        "    for i in range(num_batches):\n",
        "      end_index = min(batch_size * (i+1), len(train_set))\n",
        "\n",
        "      batch = train_set[i*batch_size:end_index]\n",
        "\n",
        "      if len(batch) == 0: continue\n",
        "\n",
        "      input_id_tensors = torch.stack([data[0] for data in batch])\n",
        "      input_mask_tensors = torch.stack([data[1] for data in batch])\n",
        "      label_tensors = torch.stack([data[2] for data in batch])\n",
        "\n",
        "      # Move tensors to the GPU\n",
        "      b_input_ids = input_id_tensors.to(device)\n",
        "      b_input_mask = input_mask_tensors.to(device)\n",
        "      b_labels = label_tensors.to(device)\n",
        "\n",
        "      # Clear the previously calculated gradient\n",
        "      model.zero_grad()        \n",
        "\n",
        "      # Perform a forward pass (evaluate the model on this training batch).\n",
        "      outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask, \n",
        "                            labels=b_labels)\n",
        "      loss = outputs.loss\n",
        "      logits = outputs.logits\n",
        "\n",
        "      total_train_loss += loss.item()\n",
        "\n",
        "      # Perform a backward pass to calculate the gradients.\n",
        "      loss.backward()\n",
        "\n",
        "      # Update parameters and take a step using the computed gradient.\n",
        "      optimizer.step()\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set. Implement this function in the cell above.\n",
        "    print(f\"Total loss: {total_train_loss}\")\n",
        "    val_acc = get_validation_performance(val_set)\n",
        "    print(f\"Validation accuracy: {val_acc}\")\n",
        "    \n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 20 ========\n",
            "Training...\n",
            "Total loss: 1.895131528377533\n",
            "Validation accuracy: 0.5833333333333334\n",
            "\n",
            "======== Epoch 2 / 20 ========\n",
            "Training...\n",
            "Total loss: 1.7595832347869873\n",
            "Validation accuracy: 0.6666666666666666\n",
            "\n",
            "======== Epoch 3 / 20 ========\n",
            "Training...\n",
            "Total loss: 1.6575713753700256\n",
            "Validation accuracy: 0.8333333333333334\n",
            "\n",
            "======== Epoch 4 / 20 ========\n",
            "Training...\n",
            "Total loss: 1.4999759793281555\n",
            "Validation accuracy: 0.8333333333333334\n",
            "\n",
            "======== Epoch 5 / 20 ========\n",
            "Training...\n",
            "Total loss: 1.294845700263977\n",
            "Validation accuracy: 0.8333333333333334\n",
            "\n",
            "======== Epoch 6 / 20 ========\n",
            "Training...\n",
            "Total loss: 1.1287816166877747\n",
            "Validation accuracy: 0.8333333333333334\n",
            "\n",
            "======== Epoch 7 / 20 ========\n",
            "Training...\n",
            "Total loss: 0.9871845245361328\n",
            "Validation accuracy: 0.9166666666666666\n",
            "\n",
            "======== Epoch 8 / 20 ========\n",
            "Training...\n",
            "Total loss: 0.8789694011211395\n",
            "Validation accuracy: 0.9166666666666666\n",
            "\n",
            "======== Epoch 9 / 20 ========\n",
            "Training...\n",
            "Total loss: 0.7518913596868515\n",
            "Validation accuracy: 0.8333333333333334\n",
            "\n",
            "======== Epoch 10 / 20 ========\n",
            "Training...\n",
            "Total loss: 0.7161953747272491\n",
            "Validation accuracy: 0.9166666666666666\n",
            "\n",
            "======== Epoch 11 / 20 ========\n",
            "Training...\n",
            "Total loss: 0.636862263083458\n",
            "Validation accuracy: 0.9166666666666666\n",
            "\n",
            "======== Epoch 12 / 20 ========\n",
            "Training...\n",
            "Total loss: 0.5531866401433945\n",
            "Validation accuracy: 0.9166666666666666\n",
            "\n",
            "======== Epoch 13 / 20 ========\n",
            "Training...\n",
            "Total loss: 0.5046624541282654\n",
            "Validation accuracy: 0.9166666666666666\n",
            "\n",
            "======== Epoch 14 / 20 ========\n",
            "Training...\n",
            "Total loss: 0.4598887115716934\n",
            "Validation accuracy: 0.9166666666666666\n",
            "\n",
            "======== Epoch 15 / 20 ========\n",
            "Training...\n",
            "Total loss: 0.413455605506897\n",
            "Validation accuracy: 0.9166666666666666\n",
            "\n",
            "======== Epoch 16 / 20 ========\n",
            "Training...\n",
            "Total loss: 0.37805285304784775\n",
            "Validation accuracy: 0.9166666666666666\n",
            "\n",
            "======== Epoch 17 / 20 ========\n",
            "Training...\n",
            "Total loss: 0.3622017949819565\n",
            "Validation accuracy: 0.9166666666666666\n",
            "\n",
            "======== Epoch 18 / 20 ========\n",
            "Training...\n",
            "Total loss: 0.3375605642795563\n",
            "Validation accuracy: 0.9166666666666666\n",
            "\n",
            "======== Epoch 19 / 20 ========\n",
            "Training...\n",
            "Total loss: 0.3147255554795265\n",
            "Validation accuracy: 0.9166666666666666\n",
            "\n",
            "======== Epoch 20 / 20 ========\n",
            "Training...\n",
            "Total loss: 0.3046092018485069\n",
            "Validation accuracy: 0.9166666666666666\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9DpRJE5mHkO"
      },
      "source": [
        "# Evaluate your model on the test set\n",
        "After you're satisfied with your hyperparameters (i.e., you're unable to achieve higher validation accuracy by modifying them further), it's time to evaluate your model on the test set! Run the below cell to compute test set accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msvZ78ii3cZZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9351cc4e-dede-4820-9035-1dc3e0d046ae"
      },
      "source": [
        "get_validation_performance(test_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9166666666666666"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcMT5aih8xEb"
      },
      "source": [
        "## Question 2.1 (10 points):\n",
        "Congratulations! You've now gone through the entire fine-tuning process and created a model for your downstream task. Two more questions left :) First, describe your hyperparameter selection process in words. If you based your process on any research papers or websites, please reference them. Why do you think the hyperparameters you ended up choosing worked better than others? Also, is there a significant discrepancy between your test and validation accuracy? Why do you think this is the case?\n",
        "\n",
        "### \n",
        "\n",
        "\n",
        "*   *Hyperparameter Selection Process: First I ran a baseline model with very low learning rate of order 1e-8 with good number of epochs but since the learning rate was low initially, the validation accuracy was stuck at 0.583. So, I decided to ramp up the learning rate but referred a paper before that. In the paper (https://openreview.net/pdf?id=nzpLWnVAyah and https://arxiv.org/pdf/1905.05583.pdf) authors highlight the significance of choosing a correct learning rate and batch size. I decided  to choose the same batch size of 20 mentioned in this paper and a learning rate in a similar range but a bit smaller (8e-6 instead of 2e-5 mentioned in the paper).*\n",
        "*   *Why hyperparameters worked better than others ? --> Learning rate is very important. Initially extremely small learning rate might lead to a slow convergence and the accuracy might get stuck in the local minima as the step size would be small. A very high learning rate in the finetuning process leads to the problem of catastrophic forgetting and hence a learning rate in the range of [7e-6, 5e-5] works very good in my case. Also, based on experiments, results from higher batch sizes are better. In our case, 20 is selected based on paper above. We could have selected 32 as mentioned in Devlin et. al. (arXiv preprint arXiv:1810.04805) but since we see that in our case, network starts converging around 15 batch size, we allow to train a bit more and stop at 20 batch size.*\n",
        "* *Also, is there a significant discrepancy between your test and validation accuracy?* : No\n",
        "* *Why do you think this is the case?* In our case, we can see that the validation accuracy is same as that of the test accuracy which leads us to believe that there is no overfitting which is true to a significant extent but there are certain things that must be kept in mind before using this model in actual scenarios: The vallidation and test examples are very small and hence the variance of the data could be high in some cases and thus a better strategy would be to create a bigger dataset so that we have significant number of validation and test examples with us. Also, since this validation and test data is small, variability could also be observed based on what examples are sampled at the starting of the notebook and even 1 misclassification can result in drastic shifts (~10% as our validation size is only 12). So, we should always keep these factors in our mind. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBbdMwt79fIs"
      },
      "source": [
        "## Question 2.2 (20 points):\n",
        "Finally, perform an *error analysis* on your model. This is good practice for your final project. Write some code in the below code cell to print out the text of up to five test set examples that your model gets **wrong** (if your model gets more than five test examples wrong, randomly choose five of them to print out). Then, in the following text cell, perform a qualitative analysis of these examples. See if you can figure out any reasons for errors that you observe, or if you have any informed guesses (e.g., common linguistic properties of these particular examples). Does this suggest any possible future steps could for your classifier?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X72mumhI9WdR"
      },
      "source": [
        "## YOUR ERROR ANALYSIS CODE HERE\n",
        "## print out up to 5 test set examples that your model gets wrong"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_z6vhO6E2c8"
      },
      "source": [
        "import numpy as np\n",
        "def get_valpred_labels(val_set):\n",
        "    # Put the model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "\n",
        "    num_batches = int(len(val_set)/batch_size) + 1\n",
        "\n",
        "    total_correct = 0\n",
        "\n",
        "    for i in range(num_batches):\n",
        "\n",
        "      end_index = min(batch_size * (i+1), len(val_set))\n",
        "\n",
        "      batch = val_set[i*batch_size:end_index]\n",
        "      \n",
        "      if len(batch) == 0: continue\n",
        "\n",
        "      input_id_tensors = torch.stack([data[0] for data in batch])\n",
        "      input_mask_tensors = torch.stack([data[1] for data in batch])\n",
        "      label_tensors = torch.stack([data[2] for data in batch])\n",
        "      \n",
        "      # Move tensors to the GPU\n",
        "      b_input_ids = input_id_tensors.to(device)\n",
        "      b_input_mask = input_mask_tensors.to(device)\n",
        "      b_labels = label_tensors.to(device)\n",
        "        \n",
        "      # Tell pytorch not to bother with constructing the compute graph during\n",
        "      # the forward pass, since this is only needed for backprop (training).\n",
        "      with torch.no_grad():        \n",
        "\n",
        "        # Forward pass, calculate logit predictions.\n",
        "        outputs = model(b_input_ids, \n",
        "                                token_type_ids=None, \n",
        "                                attention_mask=b_input_mask,\n",
        "                                labels=b_labels)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "        \n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the number of correctly labeled examples in batch\n",
        "        pred_flat = np.argmax(logits, axis=1).flatten()\n",
        "        labels_flat = label_ids.flatten()\n",
        "        num_correct = np.sum(pred_flat == labels_flat)\n",
        "        total_correct += num_correct\n",
        "        return labels_flat, pred_flat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNxT7IVgFYr2",
        "outputId": "9b0a9b0e-e7e0-45ed-8ea1-fd53fdeae543"
      },
      "source": [
        "labels_actual, pred_labels = get_valpred_labels(test_set)\n",
        "index_list = [] #Find index of non-matching labels and append it in this\n",
        "for i in range(len(labels_actual)):\n",
        "  if labels_actual[i]!=pred_labels[i]:\n",
        "    index_list.append(i)\n",
        "print(\"Non-Matching test set predictions are: \")\n",
        "for i in range(len(index_list)):\n",
        "  print(\" \")\n",
        "  print(\"Incorrect Sentence #\"+ str(i+1) + \" is: \")\n",
        "  print(\" \")\n",
        "  print(test_text[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non-Matching test set predictions are: \n",
            " \n",
            "Incorrect Sentence #1 is: \n",
            " \n",
            "No one would have crossed the ocean if he could have gotten off the ship in the storm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XyBdAup-e6Z"
      },
      "source": [
        "### *DESCRIBE YOUR QUALITATIVE ANALYSIS OF THE ABOVE EXAMPLES HERE*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcbFCadLIZC-"
      },
      "source": [
        "In my case, only one of the example is incorrect. Thus based on Mohit's suggestion in Piazza post, I am explaining the possible sets of reasons below. The sentence is \"No one would have crossed the ocean if he could have gotten off the ship in the storm\". We see in this example, there is mention of pronoun \"he\" which gives it a personal touch and might have confused the model to some extent but since there is only one incorrect example, the actual reasoning is very hard to ascertain. There could be multiple reasons as to why our model is performing well and I detail them in the upcoming pointers:\n",
        "\n",
        "*   If we see annotation guideline, the sentence is not promotional, neither comes from social media, nor is abstract and rather has a slight personal touch. Though for a human based on annotation guideline no 4, it is clearly not for students and a random sentence. While it is easy for a human annotator, it might have been difficult for the model to comprehend and classify accurately. Still, the model does extremely well on all the other examples. \n",
        "*   Since, we are testing only on 12 examples, we can say that we might have little confidence in our results and results would slightly also depend on what we sampled as a part of train, test and validate split.\n",
        "* Another, important reason is how we select the data also influences the entire accuracy of the pipeline. It is important that we choose tough examples for the model so that it is able to classify correctly even in tough cases. Model might have caught on to certain keywords and hence it became easier to classify two classes easily. Hence, present selection of validation and testing examples are way too easy for the model to classify incorrectly. ( Or probably how I filter mails in my actual life i.e. annotation guidelines are way too sorted :) )\n",
        "* Present selection of model and its hyperparameters are very good. We see that at extremely low learning rate the accuracy falls to 58.3%. Also, we know that due to the multiheaded self-attention mechanism, BERT is able to create very good contextual embedding and thus in this case able to generate a very good decision boundary which leads to a very good classification performance.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szIkBDiQ_Mkv"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Finished? Remember to upload the PDF file of this notebook to Gradescope **AND** email your three dataset files (annotator1.csv, annotator2.csv, and final_data.csv) to cs685instructors@gmail.com with the subject line formatted as **Firstname_Lastname_HW1data**.\n"
      ]
    }
  ]
}